get_ipython().getoutput("nvidia-smi")


# Load Git folder
import os
from getpass import getpass
import urllib
repo_user = 'nicolas-dufour'
user = input('Github Username: ')
password = getpass('Password: ')
repo_name = 'time-series-representation-learning'
# your password is converted into url format
password = urllib.parse.quote(password)
cmd_string = 'git clone https://{0}:{1}@github.com/{2}/{3}.git'.format(user, password, repo_user, repo_name)
os.system(cmd_string)
cmd_string, password = "", "" # removing the password from the variable
# Bad password fails silently so make sure the repo was copied
assert os.path.exists(f"/content/{repo_name}"), "Incorrect Password or Repo Not Found, please try again"


get_ipython().run_cell_magic("capture", "", """!pip install git+https://github.com/PyTorchLightning/pytorch-lightning
!pip install wandb
!pip install pyts""")


get_ipython().run_cell_magic("capture", "", """!wget https://www.cs.ucr.edu/~eamonn/time_series_data_2018/UCRArchive_2018.zip
!unzip -P 'someone' UCRArchive_2018.zip -d /content/time-series-representation-learning/
!mv /content/time-series-representation-learning/UCRArchive_2018 /content/time-series-representation-learning/data
!rm UCRArchive_2018.zip""")


get_ipython().run_cell_magic("capture", "", """!wget https://www.cs.ucr.edu/~eamonn/time_series_data_2018/UCRArchive_2018.zip
!unzip -P 'someone' UCRArchive_2018.zip
!mv UCRArchive_2018 data
!rm UCRArchive_2018.zip""")


get_ipython().run_line_magic("cd", " /content/time-series-representation-learning/")


get_ipython().run_line_magic("load_ext", " autoreload")
get_ipython().run_line_magic("autoreload", " 2")

import argparse
import json
import os
import sys
from tqdm.notebook import tqdm
import wandb

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytorch_lightning as pl
import sklearn
import torch

from datamodule import TimeSeriesDataModule
from loss import TripletLoss
from model import (
    CausalCNN,
    CausalCNNEncoder,
    CausalConvolutionBlock,
    Chomp1d,
    SqueezeChannels,
)
from train import TimeSeriesEmbedder
from utils import load_UCR_dataset
from scorer import compute_accuracy

root_data = "./data/"

# The data folder contains the data such that there are:
# ./data/DodgerLoopDay/DodgerLoopDay_TEST.tsv
# ./data/DodgerLoopDay/DodgerLoopDay_TRAIN.tsv


X_train, y_train, X_test, y_test = load_UCR_dataset(root_data, "DodgerLoopDay")

print("X_train: {}".format(X_train.shape))
print("y_train: {}".format(y_train.shape))
print("X_test: {}".format(X_test.shape))
print("y_test: {}".format(y_test.shape))


# Optimization parameters
batch_size = 10
num_workers = 2
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter
in_channels = 1
channels = 40
depth = 10
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 288

# Data parameters
train_path = os.path.join(root_data, "FordA", "FordA_TRAIN.tsv")
val_path = os.path.join(root_data, "FordA", "FordA_TEST.tsv")

train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')
val_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TEST.tsv')


# Datamodule importation
datamodule = TimeSeriesDataModule(
    train_path,
    val_path,
    batch_size,
    num_workers,
    min_length=20,
    multivariate=False,
    fill_na=True,
)
datamodule.setup()
# Model definition
model = TimeSeriesEmbedder(
    in_channels=in_channels,
    channels=channels,
    depth=depth,
    reduced_size=reduced_size,
    out_channels=out_channels,
    kernel_size=kernel_size,
    lr=lr,
    weight_decay=weight_decay,
    betas=betas,
    train_path=train_path,
    test_path=val_path
)


max_steps = 20
checkpoint_callback = pl.callbacks.ModelCheckpoint(
    mode="min",
    monitor="train_loss_epoch",
    dirpath="checkpoints",
    filename="causalcnn-{epoch:02d}-{train_loss_epoch:.2f}",
)

wandb_logger = pl.loggers.WandbLogger(
    project="Self Supervised Time Series LEarning", name="Run n 1"
)
trainer = pl.Trainer(
    gpus=1,
    max_steps=max_steps,
    num_sanity_val_steps = -1,
    logger=wandb_logger,
    # val_check_interval=50,
    callbacks=[checkpoint_callback],
)


trainer.fit(model, datamodule)


trainer.validate(model)


trainer.test()


import plotly.express as px
px.scatter(model.val_tsne_rep, x="x", y="y", color="labels", animation_frame="step")


trainer.test()


model.compute_scores()


from datamodule import UnivariateTestDataset
from sklearn.manifold import TSNE
import plotly.express as px


train_set = UnivariateTestDataset(val_path, fill_na=True)
embs = model.encoder(torch.Tensor(train_set.time_series[:,None,:])).detach().numpy()
labels = train_set.labels

tsne = TSNE(n_components=2, random_state=21)
projected_emb = pd.DataFrame(np.concatenate((labels[:,None], tsne.fit_transform(embs)), axis=1), columns = ['labels', 'x', 'y'])
projected_emb['labels'] = projected_emb['labels'].astype('int').astype('str')
fig = px.scatter(projected_emb, x = 'x', y='y', color='labels')
fig.write_html("./test_tsne_FordA_without_Train.html", auto_play=True)





from glob import glob

# Optimization parameters
batch_size = 10
num_workers = 2
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter
in_channels = 1
channels = 40
depth = 10
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 288
loss = "logi"
temp = 1

dataset_name = []
train_score_list = []
test_score_list = []

for dir_path in tqdm(glob(os.path.join(root_data, "*"))):

    print("Doing {}".format(os.path.basename(dir_path)))
    train_path = (
        glob(os.path.join(dir_path, "*TRAIN.tsv"))[0]
        if len(glob(os.path.join(dir_path, "*TRAIN.tsv"))) > 0
        else None
    )
    test_path = (
        glob(os.path.join(dir_path, "*TEST.tsv"))[0]
        if len(glob(os.path.join(dir_path, "*TEST.tsv"))) > 0
        else None
    )

    if (
        train_path is None
        or test_path is None
        or os.path.basename(dir_path) == "PigAirwayPressure"
    ):
        continue

    # Datamodule importation
    datamodule = TimeSeriesDataModule(
        train_path,
        test_path,
        batch_size,
        num_workers,
        min_length=20,
        multivariate=False,
        fill_na=True,
    )
    # Model definition
    model = TimeSeriesEmbedder(
        in_channels=in_channels,
        channels=channels,
        depth=depth,
        reduced_size=reduced_size,
        out_channels=out_channels,
        kernel_size=kernel_size,
        lr=lr,
        weight_decay=weight_decay,
        betas=betas,
        loss=loss,
        temp=temp,
    )

    max_steps = 2000
    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        mode="min",
        monitor="train_loss_epoch",
        dirpath="checkpoints",
        filename="causalcnn-{epoch:02d}-{train_loss_epoch:.2f}",
    )

    run = wandb.init(
        project="self-supervised-time-series",
        name="ALL EXPERIMENTS - {}".format(os.path.basename(dir_path)),
        tags=[os.path.basename(dir_path)],
        group="Sigmoid loss",
        entity="nicolas-dufour-team",
        reinit=True,
    )
    wandb_logger = pl.loggers.WandbLogger()
    trainer = pl.Trainer(
        gpus=1,
        max_steps=max_steps,
        num_sanity_val_steps=-1,
        logger=wandb_logger,
        # val_check_interval=50,
        callbacks=[checkpoint_callback],
    )
    model.datamodule = datamodule
    trainer.fit(model, datamodule)
    fig = trainer.test(verbose=False)

    # try:
    train_score, test_score = compute_accuracy(model, datamodule)
    print("\n", train_score, test_score)
    dataset_name.append(os.path.basename(dir_path))
    wandb.log({"Train score": train_score})
    wandb.log({"Test score": test_score})
    train_score_list.append(train_score)
    test_score_list.append(test_score)
    # except:
    #     print('Dataset: {} does not have enough example per class (will fix that)'.format(os.path.basename(dir_path)))
    run.finish()
results = pd.DataFrame(
    np.array([dataset_name, train_score_list, test_score_list]).transpose(),
    columns=["Name", "Train Accuracy", "Test Accuracy"],
)
results.to_csv("./results_univariate_without_training.csv")


from glob import glob

# Optimization parameters
batch_size = 10
num_workers = 2
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter
in_channels = 1
channels = 40
depth = 10
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 288
loss = "xent"
temp = 0.5

dataset_name = []
train_score_list = []
test_score_list = []

train_path = os.path.join(root_data, "FordA/FordA_TRAIN.tsv")
test_path = os.path.join(root_data, "FordA/FordA_TEST.tsv")

# Datamodule importation
datamodule = TimeSeriesDataModule(
    train_path,
    test_path,
    batch_size,
    num_workers,
    min_length=20,
    multivariate=False,
    fill_na=True,
)
# Model definition
model = TimeSeriesEmbedder(
    in_channels=in_channels,
    channels=channels,
    depth=depth,
    reduced_size=reduced_size,
    out_channels=out_channels,
    kernel_size=kernel_size,
    lr=lr,
    weight_decay=weight_decay,
    betas=betas,
    loss=loss,
    temp=temp,
)

trainer = pl.Trainer(gpus=1, max_steps=max_steps,)
trainer.fit(model, datamodule)

for dir_path in tqdm(glob(os.path.join(root_data, "*"))):

    print("Doing {}".format(os.path.basename(dir_path)))
    train_path = (
        glob(os.path.join(dir_path, "*TRAIN.tsv"))[0]
        if len(glob(os.path.join(dir_path, "*TRAIN.tsv"))) > 0
        else None
    )
    test_path = (
        glob(os.path.join(dir_path, "*TEST.tsv"))[0]
        if len(glob(os.path.join(dir_path, "*TEST.tsv"))) > 0
        else None
    )

    if (
        train_path is None
        or test_path is None
        or os.path.basename(dir_path) == "PigAirwayPressure"
    ):
        continue

    # Datamodule importation
    datamodule = TimeSeriesDataModule(
        train_path,
        test_path,
        batch_size,
        num_workers,
        min_length=20,
        multivariate=False,
        fill_na=True,
    )
    run = wandb.init(
        project="self-supervised-time-series",
        name="Transfert - {}".format(os.path.basename(dir_path)),
        tags=[os.path.basename(dir_path)],
        group="Transfert from FordA",
        entity="nicolas-dufour-team",
        reinit=True,
    )
    # try:
    train_score, test_score = compute_accuracy(model, datamodule)
    print("\n", train_score, test_score)
    dataset_name.append(os.path.basename(dir_path))
    wandb.log({"Train score": train_score})
    wandb.log({"Test score": test_score})
    train_score_list.append(train_score)
    test_score_list.append(test_score)
    # except:
    #     print('Dataset: {} does not have enough example per class (will fix that)'.format(os.path.basename(dir_path)))
    run.finish()
results = pd.DataFrame(
    np.array([dataset_name, train_score_list, test_score_list]).transpose(),
    columns=["Name", "Train Accuracy", "Test Accuracy"],
)
results.to_csv("./results_univariate_without_training.csv")





# Optimization parameters 
batch_size = 20
num_workers = 4
betas = (0.9, 0.999)
weight_decay = 1e-2
lr = 0.001

# Model parameter 
in_channels = 9
channels = 40
depth = 7
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 288

# Data parameters
train_path = os.path.join(root_data,'FordA','FordA_TRAIN.tsv')
val_path = os.path.join(root_data,'FordA','FordA_TEST.tsv')
dataset_name='ArticularyWordRecognition'

# Datamodule importation
datamodule = TimeSeriesDataModule('', '', batch_size, num_workers, dataset_name, min_length=20, multivariate=True, fill_na=True)
datamodule.setup()

# Model definition
model = TimeSeriesEmbedder(
    in_channels = in_channels,
    channels = channels,
    depth = depth,
    reduced_size = reduced_size,
    out_channels = out_channels,
    kernel_size = kernel_size,
    lr=lr,
    weight_decay = weight_decay,
    betas = betas,
    train_path=train_path,
    test_path=val_path, 
    dataset_name=dataset_name,
    multivariate=True)


max_steps = 300
checkpoint_callback = pl.callbacks.ModelCheckpoint(
    mode="min",
    monitor="train_loss_epoch",
    dirpath="checkpoints",
    filename="causalcnn-{epoch:02d}-{train_loss_epoch:.2f}",
)

wandb_logger = pl.loggers.WandbLogger(
    project="Self Supervised Time Series LEarning", name="Run n 1 - Multivariate"
)
trainer = pl.Trainer(
    gpus=1,
    max_steps=max_steps,
    num_sanity_val_steps = -1,
    logger=wandb_logger,
    #val_check_interval=50,
    callbacks=[checkpoint_callback],
)


trainer.fit(model, datamodule)


trainer.validate(model)


trainer.test()


import plotly.express as px
px.scatter(model.val_tsne_rep, x="x", y="y", color="labels", animation_frame="step")



model.compute_scores()





datamodule.setup(0)
train_dataloader = datamodule.train_dataloader()
test_dataloader = datamodule.test_dataloader()

labels, test_time_series = next(iter(test_dataloader))
ref_time_series, pos_time_series = next(iter(train_dataloader))


pos_time_series.shape


ref_time_series.shape


from pyts.datasets import load_basic_motions, uea_dataset_list, fetch_uea_dataset
uea_dataset_list()

fetch_uea_dataset(dataset = uea_dataset_list()[0])


datamodule = TimeSeriesDataModule('', '', batch_size, num_workers, dataset_name, min_length=20, multivariate=True, fill_na=True)
datamodule.setup()

datamodule.train_set.time_series.shape[1]





in_channels = 6
channels = 40
depth = 4
reduced_size = 160
out_channels = 320
kernel_size = 3
N_sample = 100

# The whole model
causalEncoder = CausalCNNEncoder(
    in_channels=in_channels,
    channels=channels,
    depth=depth,
    reduced_size=reduced_size,
    out_channels=out_channels,
    kernel_size=kernel_size,
).double()

# The whole model without the last global average pooling and FC between reduced_size and out_channel
causal_cnn = CausalCNN(
    in_channels=in_channels,
    channels=channels,
    depth=depth,
    out_channels=out_channels,
    kernel_size=kernel_size,
).double()


##### BUILDING EACH BLOCK OF THE MODEL

# Each of the Causal CNN Blocks (of width 4)
ConvBlock_1 = CausalConvolutionBlock(
    in_channels=in_channels, out_channels=channels, dilation=1, kernel_size=3
).double()
ConvBlock_2 = CausalConvolutionBlock(
    in_channels=channels, out_channels=channels, dilation=2, kernel_size=3
).double()
ConvBlock_3 = CausalConvolutionBlock(
    in_channels=channels, out_channels=channels, dilation=4, kernel_size=3
).double()
ConvBlock_4 = CausalConvolutionBlock(
    in_channels=channels, out_channels=reduced_size, dilation=8, kernel_size=3
).double()

# Global average pooling
reduce_size = torch.nn.AdaptiveMaxPool1d(1)

# Squeez the last (third) temporal dimension
squeeze = SqueezeChannels()

# last fully connected layer to go from reduced_size to out_channel
linear = torch.nn.Linear(reduced_size, out_channels).double()


model_from_scratch = [
    ConvBlock_1,
    ConvBlock_2,
    ConvBlock_3,
    ConvBlock_4,
    reduce_size,
    squeeze,
    linear,
]
model_name_from_scratch = [
    "ConvBlock_1",
    "ConvBlock_2",
    "ConvBlock_3",
    "ConvBlock_4",
    "Global Average Pooling",
    "squeezing",
    "final FC",
]

print("Input Shape:")
print(list(X_train.shape), "\n")
input = X_train
for block_ii in range(len(model_from_scratch)):
    print("{}:".format(model_name_from_scratch[block_ii]))
    output = model_from_scratch[block_ii](input)
    print(list(output.shape), "\n")
    input = output


# Instead of the Global Average Pooling
linear_to_squeeze = torch.nn.Linear(N_sample, 1).double()


model_from_scratch_experiment = [
    ConvBlock_1,
    ConvBlock_2,
    ConvBlock_3,
    ConvBlock_4,
    linear_to_squeeze,
    squeeze,
    linear,
]
model_name_from_scratch_experiment = [
    "ConvBlock_1",
    "ConvBlock_2",
    "ConvBlock_3",
    "ConvBlock_4",
    "linear_to_squeeze",
    "squeezing",
    "final FC",
]

print("Input Shape:")
print(list(X_train.shape), "\n")
input = X_train
for block_ii in range(len(model_from_scratch_experiment)):
    print("{}:".format(model_name_from_scratch_experiment[block_ii]))
    output = model_from_scratch_experiment[block_ii](input)
    print(list(output.shape), "\n")
    input = output


X_train, y_train, X_test, y_test = load_UCR_dataset(root_data,'DodgerLoopDay')

print("X_train: {}".format(X_train.shape))
print("y_train: {}".format(y_train.shape))
print("X_test: {}".format(X_test.shape))
print("y_test: {}".format(y_test.shape))

print('Percentage of Nan in the training set: {}'.format(100*np.isnan(X_train).sum()/(X_train.shape[0]*X_train.shape[2])))
fig,(ax1,ax2) = plt.subplots(2,1, figsize=(20,8))
fig.suptitle('Time serie before and after replaing NaN Value', weight='bold', fontsize=18)
ax1.plot(X_train[16,0,:], label='Original time serie')
nan_mask = np.isnan(X_train)
X_train[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))
ax2.plot(X_train[16,0,:], label='Time serie transformed')
ax1.legend()
_ = ax2.legend()


train_path = os.path.join(root_data,'DodgerLoopDay','DodgerLoopDay_TRAIN.tsv')
fill_na = True
min_length = 20

data = pd.read_csv(train_path,sep='\t', header=None)
time_series = np.array(data.iloc[:,1:])

if fill_na:
    #print('Percentage of Nan in the training set: {:.2f}\nRemoving nan...'.format(100*np.isnan(self.time_series).sum()/(self.time_series.shape[0]*self.time_series.shape[1])))
    nan_mask = np.isnan(time_series)
    time_series[nan_mask] = np.zeros(shape=np.count_nonzero(nan_mask))

    
idx=0

entire_series = time_series[idx]
entire_length = entire_series.shape[0]
pos_length = np.random.randint(min_length, high = entire_length+1)
ref_length = np.random.randint(pos_length, high = entire_length+1)
ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)
ref_series = entire_series[ref_beg:ref_beg+ref_length]
pos_series = entire_series[pos_beg:pos_beg+pos_length]

# Visualization
plt.figure(figsize=(30,8))
plt.title('Example time serie, with reference and positive samples', weight='bold', fontsize=20)
plt.plot(entire_series, color='k', label='Original Time Series')
plt.plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')
plt.plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')
plt.xlabel('Time')
_ = plt.legend(prop={'size':20})


from pyts.datasets import load_basic_motions,uea_dataset_list, fetch_uea_dataset
X_train, X_test, y_train, y_test = fetch_uea_dataset('BasicMotions', use_cache=False, data_home='./data/Multivariate/', return_X_y=True)


# Preprocessing: normalization
X_train = (X_train - X_train.mean(axis=2)[:, :, None]) / X_train.std(axis=2)[:, :, None]
X_test = (X_test - X_test.mean(axis=2)[:, :, None]) / X_test.std(axis=2)[:, :, None]


X_train = torch.from_numpy(X_train).double()
X_test = torch.from_numpy(X_test).double()
if torch.cuda.is_available():
    X_train = X_train.cuda()
    X_test = X_test.cuda()
    
    
labels = ["Accelerometer - X", "Accelerometer - Y", "Accelerometer - Z", "Gyroscope - Yaw", "Gyroscope - Pitch", "Gyroscope - Roll"]
if True:
    index = 0
    fig = plt.figure(figsize=(30,15))
    axes = []
    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*i+1))
        axes[i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        axes[i].plot(X_train[index,i,:], label = labels[i])
        axes[i].legend(loc=1)
    index = -1
    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*(i+1)))
        axes[6+i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        axes[6+i].plot(X_train[index,i,:], label = labels[i])
        axes[6+i].legend(loc=1)
plt.tight_layout()



time_series = X_train


idx=0
min_length = 20
entire_series = time_series[idx]
entire_length = entire_series.shape[1]
pos_length = np.random.randint(min_length, high = entire_length+1)
ref_length = np.random.randint(pos_length, high = entire_length+1)
ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)

ref_series = entire_series[:,ref_beg:ref_beg+ref_length]
pos_series = entire_series[:,pos_beg:pos_beg+pos_length]



labels = ["Accelerometer - X", "Accelerometer - Y", "Accelerometer - Z", "Gyroscope - Yaw", "Gyroscope - Pitch", "Gyroscope - Roll"]
if True:
    index = 0
    min_length = 20
    entire_series = time_series[index]
    entire_length = entire_series.shape[1]
    pos_length = np.random.randint(min_length, high = entire_length+1)
    ref_length = np.random.randint(pos_length, high = entire_length+1)
    ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
    pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)

    ref_series = entire_series[:,ref_beg:ref_beg+ref_length]
    pos_series = entire_series[:,pos_beg:pos_beg+pos_length]

    fig = plt.figure(figsize=(30,15))
    axes = []
    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*i+1))
        axes[i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        #axes[i].plot(X_train[index,i,:], label = labels[i])
        axes[i].plot(entire_series[i,:], color='k', label='Entire Serie')
        axes[i].plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[i,ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')
        axes[i].plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[i,pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')
        axes[i].legend(loc=1)
        
    index = -1
    min_length = 20
    entire_series = time_series[index]
    entire_length = entire_series.shape[1]
    pos_length = np.random.randint(min_length, high = entire_length+1)
    ref_length = np.random.randint(pos_length, high = entire_length+1)
    ref_beg = np.random.randint(0, high = entire_length+1-ref_length)
    pos_beg = np.random.randint(ref_beg, high = ref_beg+ref_length-pos_length+1)
    ref_series = entire_series[:,ref_beg:ref_beg+ref_length]
    pos_series = entire_series[:,pos_beg:pos_beg+pos_length]

    for i in range(6):
        axes.append(fig.add_subplot(6,2,2*(i+1)))
        axes[6+i].set_title('{}'.format(str(y_train[index])[2:-1]), weight = 'bold', fontsize=18)
        #axes[6+i].plot(X_train[index,i,:], label = labels[i])
        axes[6+i].plot(entire_series[i,:], color='k', label='Entire Serie')
        axes[6+i].plot(np.arange(ref_beg,ref_beg+ref_length),entire_series[i,ref_beg:ref_beg+ref_length], color='b', label='Ref Time Series')
        axes[6+i].plot(np.arange(pos_beg,pos_beg+pos_length),entire_series[i,pos_beg:pos_beg+pos_length], color='r', label='Pos Time Series')
        axes[6+i].legend(loc=1)
plt.tight_layout()








# Save to git
get_ipython().getoutput("git config --global user.email "nicolas.dufourn@gmail.com"")
get_ipython().getoutput("git config --global user.name "Nicolas DUFOUR"")
get_ipython().getoutput("git add --all")
get_ipython().getoutput("git commit -m "Added batchs to compute_score"")
get_ipython().getoutput("git push")


get_ipython().getoutput("git pull")



